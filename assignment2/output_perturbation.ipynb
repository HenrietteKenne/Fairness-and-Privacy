{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Private Training by Output Perturbation.\"\"\"\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "import torch\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import nn\n",
    "\n",
    "from logistic_regression import nonprivate_logistic_regression\n",
    "from utils import get_data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gamma_sample_pytorch_parameterization(concentration, rate):\n",
    "    \"\"\"The Gamma dist'n as it is parameterized in PyTorch\"\"\"\n",
    "    return Gamma(concentration, rate).sample()\n",
    "\n",
    "\n",
    "def gamma_sample_chaudhuri_parameterization(concentration, scale):\n",
    "    \"\"\"The Gamma dist'n as it is parameterized in Chaudhuri and Monteleoni\"\"\"\n",
    "    rate = 1. / scale\n",
    "    return gamma_sample_pytorch_parameterization(concentration, rate)\n",
    "\n",
    "\n",
    "def random_unit_norm_vector(num_dims):\n",
    "    random_rotation_matrix = ortho_group.rvs(num_dims)\n",
    "    basis_vector_one = np.eye(num_dims)[0]\n",
    "    vector = np.matmul(random_rotation_matrix, basis_vector_one)\n",
    "    return torch.tensor(vector, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def private_logistic_regression(dset_loader, num_epochs, learning_rate,\n",
    "    lmbda, epsilon, seed=None):\n",
    "    ############################################################################\n",
    "    # TODO(student)\n",
    "    #\n",
    "    # your code here...\n",
    "    #\n",
    "    # hint: use the code we have given you. For example you don't have to \n",
    "    # implement non-private logistic regression from scratch because an \n",
    "    # implementation exists in logistic_regression.py. There are also functions \n",
    "    # in this file for sampling Laplace noise\n",
    "    #\n",
    "    # hint: the input dim d can be found as a attr of the dset_loader's dset\n",
    "    #       >>> num_pixels = dset_loader.dataset.num_pixels\n",
    "    #\n",
    "        \n",
    "    num_pixels = dset_loader.dataset.num_pixels\n",
    "    \n",
    "    n = len(dset_loader.dataset)\n",
    "    \n",
    "    scale = 2/(n*epsilon*lmbda)\n",
    "    \n",
    "    noise_dist = random_unit_norm_vector(num_pixels)\n",
    "    \n",
    "    noise_norm = gamma_sample_chaudhuri_parameterization(num_pixels,scale)\n",
    "    \n",
    "    noise = noise_dist*noise_norm\n",
    "    \n",
    "    nonprivate_params = nonprivate_logistic_regression(dset_loader, num_epochs, learning_rate,lmbda, seed)\n",
    "    \n",
    "    weight_private = nonprivate_params['weight']+ noise\n",
    "    \n",
    "    private_params = {\n",
    "        'weight': weight_private,  # replace me (but this is how to format the state_dict)\n",
    "        }\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    ############################################################################\n",
    "\n",
    "    \n",
    "    return private_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(n, epsilon, lmbda, epochs, batch_size, lr, data_seed, model_seed):\n",
    "    # load data\n",
    "    loaders, _ = get_data_loaders(data_seed, batch_size, n)\n",
    "    loaders.pop('neighbor')  # don't need this loader for this question\n",
    "  \n",
    "    # train model\n",
    "    nonprivate_params = \\\n",
    "            nonprivate_logistic_regression(loaders['train'], epochs, \n",
    "                    lr, lmbda, seed=model_seed)\n",
    "  \n",
    "    private_params = private_logistic_regression(loaders['train'], epochs, \n",
    "        lr, lmbda, epsilon, seed=model_seed)\n",
    "  \n",
    "    # evaluate\n",
    "    test_losses = dict()\n",
    "    test_accs = dict()\n",
    "    for name, params in zip(['nonprivate', 'private'], \n",
    "          [nonprivate_params, private_params]):\n",
    "        num_pixels = loaders['train'].dataset.num_pixels\n",
    "        model = nn.Linear(num_pixels, 1, bias=False)\n",
    "        criterion = nn.BCEWithLogitsLoss()  # binary cross entropy\n",
    "        model.load_state_dict(params)\n",
    "        model.eval()\n",
    "        num_test_examples = len(loaders['test'].dataset)\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in loaders['test']:\n",
    "                images = images.reshape(-1, 28*28)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                test_loss += loss.item() * len(images) / float(num_test_examples)\n",
    "                predicted = (outputs.squeeze() > 0.).long()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            test_acc = float(correct) / float(total)\n",
    "            test_losses[name] = test_loss\n",
    "            test_accs[name] = 100. * test_acc  # format as a percentage\n",
    "  \n",
    "    from pprint import pprint\n",
    "    print('final test losses')\n",
    "    print('nonprivate: {nonprivate:.2f}, private: {private:.2f}'\n",
    "          .format(**test_losses))\n",
    "    print('final test accs')\n",
    "    print('nonprivate: {nonprivate:.2f}, private: {private:.2f}'\n",
    "          .format(**test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arguments and main function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.44it/s]\n",
      "100%|██████████| 100/100 [00:16<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.44, private: 0.44\n",
      "final test accs\n",
      "nonprivate: 96.00, private: 95.00\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "EPSILON = 4.\n",
    "LMBDA = 5e-2\n",
    "EPOCHS = 100  # run for more epochs once your code works\n",
    "BATCH_SIZE = 256\n",
    "LR = .1\n",
    "DATA_SEED = 0\n",
    "MODEL_SEED = 0\n",
    "\n",
    "main(N, EPSILON, LMBDA, EPOCHS, BATCH_SIZE, LR, DATA_SEED, MODEL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Question answer: Start by answering the following True/False propositions:\n",
    "1. False\n",
    "2. False\n",
    "3. False\n",
    "Because the scale  formular, when we increase lambda and epsilon the scale become small and the noise_norm is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 82.64it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 79.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.40, private: 0.86\n",
      "final test accs\n",
      "nonprivate: 100.00, private: 70.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "EPSILON = 4.\n",
    "LMBDA = 5e-2\n",
    "EPOCHS = 100  # run for more epochs once your code works\n",
    "BATCH_SIZE = 256\n",
    "LR = .1\n",
    "DATA_SEED = 0\n",
    "MODEL_SEED = 0\n",
    "\n",
    "main(N, EPSILON, LMBDA, EPOCHS, BATCH_SIZE, LR, DATA_SEED, MODEL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 87.28it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 81.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test losses\n",
      "nonprivate: 0.67, private: 0.75\n",
      "final test accs\n",
      "nonprivate: 90.00, private: 70.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "EPSILON = 4.\n",
    "LMBDA = 5e-2\n",
    "EPOCHS = 1000  # run for more epochs once your code works\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.0001\n",
    "DATA_SEED = 0\n",
    "MODEL_SEED = 0\n",
    "\n",
    "main(N, EPSILON, LMBDA, EPOCHS, BATCH_SIZE, LR, DATA_SEED, MODEL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
